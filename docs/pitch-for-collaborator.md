# Collaboration Pitch for Dr. Yang

## Proposed Title

**"Effects of Agentic AI on Learning Outcomes Across Educational Contexts: A Meta-Analysis with Implications for Human-AI Learning Orchestration"**

---

## The Idea in One Paragraph

We propose a meta-analysis of Agentic AI (AI systems with autonomous action capabilities) in education, coding **human oversight level** and **single vs. multi-agent architecture** as key moderators. Your MCP research and Dynamic Learner State theory provide the theoretical basis for multi-agent communication and state tracking variables, while my Diverga checkpoint system provides the human oversight framework. The meta-analytic evidence will directly inform the **HALO (Human-AI Learning Orchestration) Framework**, integrating MCP + Diverga into a unified, empirically-grounded design architecture.

---

## Why This Research?

1. **No meta-analysis has examined human oversight level as a moderator** -- "When should AI act autonomously vs. when should humans intervene?" remains unanswered with empirical evidence.
2. **No meta-analysis has compared single vs. multi-agent architectures** -- The empirical basis (or counter-evidence) for MCP-based multi-agent orchestration is missing.
3. **Agentic AI is rapidly expanding in education**, but design principles lag behind -- both HRD and educational technology fields need evidence-based guidelines.

---

## What Each of Us Brings

| Area | My Contribution | Your Contribution |
|------|----------------|-------------------|
| **Theoretical** | Diverga checkpoint principles (human oversight coding) | Dynamic Learner State theory (MCP-based tracking variables) |
| **Framework** | HALO Layer 3: Orchestration & Checkpoints | HALO Layer 2: Protocol & State Tracking |
| **Coding** | Coder 1 + Coding scheme management | Coder 2 (independent coding for Cohen's kappa) |
| **Analysis** | Meta-analysis execution in R, moderator analysis | Data validation, robustness checks |
| **Writing** | Framework, Discussion (Diverga implications) | Literature Review, Method (MCP theory integration) |

---

## Proposed Role Distribution

### Research Design
- **You**: Define coding variables grounded in Dynamic Learner State and MCP principles
- **Me**: Design HALO Framework structure, develop coding scheme reflecting Diverga checkpoint principles

### Search & Screening
- **Joint**: Independent screening with consensus resolution

### Coding
- **You**: Independent Coder 2
- **Me**: Independent Coder 1 + coding scheme management

### Analysis
- **Me**: Run meta-analysis (R: metafor, robumeta, clubSandwich), moderator analysis
- **You**: Data verification, robustness/sensitivity checks

### Manuscript
- **You**: Literature Review, Method sections (MCP theory integration)
- **Me**: Framework, Discussion sections (Diverga implications)

---

## Target Journal

**Educational Research Review** (IF 11.8) or **Computers & Education** (IF 11.5)
- Both have strong track records publishing meta-analyses with framework development
- HRD perspective can be highlighted in the Discussion section

---

## Timeline

**Total: ~7 months**

| Phase | Duration | Activity |
|-------|----------|----------|
| 1 | Month 1 | PROSPERO registration, search strategy finalization, pilot search |
| 2 | Month 1-2 | Systematic search, deduplication, title/abstract screening |
| 3 | Month 2-3 | Full-text screening, final study selection |
| 4 | Month 3-4 | Independent coding (2 coders), reliability verification, data extraction |
| 5 | Month 4-5 | Meta-analysis execution, moderator analysis, HALO refinement |
| 6 | Month 5-7 | Manuscript writing, internal review, revision |
| 7 | Month 7 | Submission |

---

## Expected Contributions

### Theoretical
- First meta-analytic evidence on human oversight effects in AI-assisted learning
- Empirical basis for single vs. multi-agent system effectiveness
- HALO Framework integrating MCP + Diverga with evidence-based design principles

### Practical
- Guidelines for optimal human oversight levels in AI agent deployment
- Evidence-based selection criteria for single vs. multi-agent implementations
- Design checklist for AI agents in workplace/professional learning contexts

---

## Key References

- Yang (2025). Towards Dynamic Learner State: Orchestrating AI Agents and Workplace Performance via the Model Context Protocol. *Education Sciences*, 15(8), 1004.
- Dai et al. (2024). Effects of AI-Powered Virtual Agents on Learning Outcomes in Computer-Based Simulations. *Educational Psychology Review*, 36, 31.
- Yan (2025). From Passive Tool to Socio-cognitive Teammate: APCP Framework. arXiv:2508.14825.

---

## Next Steps

1. Discuss and refine research questions together
2. Agree on coding scheme variables (especially MCP-informed variables)
3. Register protocol on PROSPERO
4. Begin pilot search to estimate study pool size
