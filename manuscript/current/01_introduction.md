# 1. Introduction

Artificial intelligence systems capable of autonomous pedagogical action -- from adaptive tutoring to peer-level collaboration -- are rapidly proliferating across educational settings. In K-12 classrooms, AI tutors adapt instruction in real time to individual learner performance; in higher education, conversational AI agents scaffold academic writing and problem-solving; in workplace training, multi-agent systems coordinate specialized roles to deliver personalized professional development (Kasneci et al., 2023; Ouyang & Jiao, 2021). This new generation of educational AI, increasingly characterized as "agentic" for its capacity to set goals, monitor progress, and initiate actions without continuous human direction, represents a qualitative shift from earlier AI tools that served primarily as content delivery or information retrieval mechanisms.

Yet this rapid deployment has outpaced the development of evidence-based design principles. A critical question, largely unaddressed by the existing evidence base, concerns the calibration of human oversight: when should these AI systems operate autonomously, when should human educators intervene, and how should this balance be configured across different learning contexts? This question has gained particular urgency following the enactment of the European Union's AI Act (2024), which mandates human oversight for high-risk AI systems -- a category that explicitly includes AI deployed in educational contexts (Article 14). Without meta-analytic evidence on how different levels of human oversight moderate AI effectiveness in education, practitioners and policymakers lack a principled basis for meeting governance requirements while maximizing learning outcomes.

The existing meta-analytic literature, while informative, leaves three significant gaps that the present study addresses. First, prior meta-analyses have focused on specific AI subtypes -- intelligent tutoring systems (Ma et al., 2014; Kulik & Fletcher, 2016), virtual agents in simulations (Dai et al., 2024), and pedagogical agents (Schroeder et al., 2013) -- rather than examining the full agency spectrum of AI systems now deployed in education. As AI capabilities have expanded from adaptive response to peer-level interaction (Yan, 2025), a synthesis that captures this full spectrum is needed. Second, no meta-analysis has examined human oversight level as a moderator of AI effectiveness, despite the theoretical and regulatory importance of this design dimension. Existing reviews have coded moderators such as subject domain, comparison condition, and study design, but the degree of human involvement during AI-mediated learning has been entirely absent from the evidence base. Third, the emergence of multi-agent AI architectures -- systems in which multiple specialized agents coordinate to support different aspects of learning -- introduces a design dimension whose effects have not been quantified at the meta-analytic level.

This study addresses these gaps through a comprehensive meta-analysis of agentic AI effects on learning outcomes across K-12, higher education, and workplace training contexts. Drawing on established automation level frameworks (Parasuraman et al., 2000) and recent educational AI taxonomies (Yan, 2025; Yang, 2025), the study introduces human oversight level as a novel moderator, examines single-agent versus multi-agent architectures, and compares effects across learning contexts. Five research questions guide the analysis: (1) the overall effect of agentic AI on learning outcomes; (2) the moderating role of human oversight level; (3) the comparative effectiveness of single-agent versus multi-agent architectures; (4) differential effects across learning contexts; and (5) the derivation of empirically grounded design principles.

The study makes two primary contributions. First, it provides the first meta-analytic examination of human oversight level as a moderator of agentic AI effectiveness in education, grounding this novel moderator in Parasuraman et al.'s (2000) established automation level taxonomy and connecting it to current AI governance requirements. Second, it derives the HALO (Human-AI Learning Orchestration) Framework -- a three-layer, evidence-based architecture that systematically maps moderator analysis results to actionable design principles for AI agent-based learning systems. By translating meta-analytic findings into a structured design framework, this study bridges the gap between empirical evidence and educational AI practice.

The remainder of this paper is organized as follows. Section 2 reviews the relevant literature and presents the theoretical foundations informing the research questions. Section 3 introduces the initial (pre-analysis) version of the HALO Framework. Section 4 describes the meta-analytic method. Section 5 reports the results. Section 6 presents the refined, evidence-based HALO Framework. Section 7 discusses theoretical and practical implications. Section 8 concludes with directions for future research.
